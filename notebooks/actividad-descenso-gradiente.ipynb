{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1093601d",
   "metadata": {},
   "source": [
    "# Descenso de Gradiente\n",
    "En esta actividad vas a implementar el descenso de gradiente y después utilizarlo para ajustar una modelo lineal a un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import graficar_contorno_con_descenso, graficar_gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])       # tamaño de casa [m^2]\n",
    "y_train = np.array([150, 250])       # valores objetivo (Precio miles de pesos) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9462a",
   "metadata": {},
   "source": [
    "Primeramente vamos a implementar una función para calcular el costo dados los paramétros $w$,$b$, la entrada $x$ y la salida real $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dabd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_costo(x, y, w, b):\n",
    "    \"\"\" \n",
    "    x ndarray (N,): Datos, N ejemplos \n",
    "    y ndarray (N,): valores objetivo\n",
    "    w,b (escalares) : parámetros del modelo  \n",
    "    \"\"\"\n",
    "    \n",
    "    N = x.shape[0] \n",
    "    costo = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Coloca aqui tu código para calcular la pérdida para cada ejemplo de entrenamiento\n",
    "\n",
    "    costo_total = 1 / (N) * costo\n",
    "\n",
    "    return costo_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458ac9e",
   "metadata": {},
   "source": [
    "Prueba tu implementación de la función costo con la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1])\n",
    "y = np.array([0.5,2.1])\n",
    "w = 1\n",
    "b = 1 \n",
    "\n",
    "# Valor esperado con los valores dados es 0.13\n",
    "costo = calcular_costo(x,y,w,b)\n",
    "print(costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cc04a",
   "metadata": {},
   "source": [
    "<a name=\"Descenso de gradiente\"></a>\n",
    "## Recordatorio del descenso de gradiente\n",
    "Recordemos que nuestro modelo lineal es:\n",
    "$$h_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "En regresión lineal, se utilizan ejemplos de entrenamiento para ajustar los parámetros $w$,$b$ minimizando una medida de error entre nuestras predicciones $\\hat{y}=h_{w,b}(x)$ y los valores reales observados $y$. Llamamos a esta medida \"costo\". En este caso estamos usando el error cuadrático medio como la función costo:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{N} \\sum\\limits_{i = 0}^{N-1} (h_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18707f67",
   "metadata": {},
   "source": [
    "\n",
    "El algoritmo de *descenso de gradiente* se describe como:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ hasta la convergencia:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "donde los parámetros $w$, $b$ se actualizan simultáneamente.  \n",
    "Así que para implementar el descenso de gradiente, necesitamos las derivadas parciales del costo respecto a los parámetros (derivamos esto en clase):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{2}{N} \\sum\\limits_{i = 0}^{N-1} (h_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{2}{N} \\sum\\limits_{i = 0}^{N-1} (h_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aqui *simultáneamente* significa que se deben calcular las derivadas parciales para todos los parámetros antes de actualizar cualquiera de los parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fa221",
   "metadata": {},
   "source": [
    "### Implementación del descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb097fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_gradiente(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Cálcula el gradiente para la regresión lineal \n",
    "    Args:\n",
    "      x ndarray (N,): Datos, N ejemplos \n",
    "      y ndarray (N,): valores objetivo\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    Returns\n",
    "      dj_dw (escalar): El gradiente de la función costo con respecto al parámetro w\n",
    "      dj_db (escalar): El gradiente de la función costo con respecto al parámetro b    \n",
    "     \"\"\"\n",
    "    \n",
    "    N = x.shape[0]    # Número de ejemplos en el conjunto de entrenamiento\n",
    "    dj_dw = 0         \n",
    "    dj_db = 0         \n",
    "    \n",
    "    for i in range(N):  \n",
    "        # Tu para calcular h_wb_i aqui\n",
    "\n",
    "        # Tu código para calcular  dj_dw_i y dj_db_i aqui\n",
    "\n",
    "        # Tu código para acumular el gradiente de todos los ejemplos de entrenamiento aqui\n",
    "\n",
    "    # Finalmente multiplicamos por 2/N\n",
    "    dj_dw = 2 * dj_dw / N \n",
    "    dj_db = 2 * dj_db / N \n",
    "        \n",
    "    return dj_dw, dj_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b62d56",
   "metadata": {},
   "source": [
    "Comprueba que tu implementación de la función gradiente es correcta. Corriendo la siguiente celda y verificando si el resultado esperado es:\n",
    "\n",
    "dj_dw = -0.1 \n",
    "dj_db = 0.399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1])\n",
    "y = np.array([0.5,2.1])\n",
    "w = 1\n",
    "b = 1 \n",
    "\n",
    "# Valor\n",
    "gradiente = calcular_gradiente(x, y, w , b)\n",
    "print(gradiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a0c17",
   "metadata": {},
   "source": [
    "# Visualizaciones previas\n",
    "\n",
    "Ahora que ya implementaste la función calcular el gradiente. Vamos a utilizarla para crear algunas visualizaciones del gradiente. No es necesario que estudies los detalles de las funciones de visualización. Lo importante es interpretar las gráficas creadas. Así que solamente corre la siguiente celda, observa los resultados e intenta descifrar que es lo que nos dicen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_gradientes(x_train, y_train, calcular_costo, calcular_gradiente)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc9413",
   "metadata": {},
   "source": [
    "La gráfica a la izquierda de la celda de arriba muestra $\\frac{\\partial J(w,b)}{\\partial w}$ que es la pendiente del costo relativo a $w$ en tres puntos. Las derivadas guian el descenso de gradiante hacia el mínimo de la función costo. \n",
    " \n",
    "A la derecha, se muestra el campo de gradientes considerando ambos parámetros. El tamaño de las flechas refleja la magnitud del gradiente en ese punto. Notar que en realidad el gradiente apunta alejandose del mínimo. Sin embargo, el signo menos en el algoritmo de descenso de gradiente cambia el sentido y mueve los parámetros en la dirección que reduce el costo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3190dfe",
   "metadata": {},
   "source": [
    "###  Descenso de gradiente\n",
    "La siguiente celda define la función descenso de gradiente que será utilizada para ajustar los valores de $w$ y $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef10b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(datos, etiquetas, w_inicial, b_inicial, tasa_aprendizaje, iteraciones, funcion_costo, funcion_gradiente):\n",
    "    \"\"\"\n",
    "    Realiza descenso del gradiente para ajustar los parámetros w y b.\n",
    "    Actualiza w y b realizando el número de iteraciones indicadas con una\n",
    "    tasa de aprendizaje específica.\n",
    "\n",
    "    Parámetros:\n",
    "      datos (ndarray (m,))         : Datos de entrada, m ejemplos\n",
    "      etiquetas (ndarray (m,))     : Valores objetivo\n",
    "      w_inicial, b_inicial (float) : Valores iniciales de los parámetros\n",
    "      tasa_aprendizaje (float)     : Tasa de aprendizaje\n",
    "      iteraciones (int)            : Número de iteraciones del algoritmo\n",
    "      funcion_costo                : Función que calcula el costo\n",
    "      funcion_gradiente            : Función que calcula los gradientes\n",
    "\n",
    "    Retorna:\n",
    "      w_final (float)              : Valor final del parámetro w\n",
    "      b_final (float)              : Valor final del parámetro b\n",
    "      historial_costos (list)      : Lista con el valor del costo por iteración\n",
    "      historial_parametros (list)  : Lista con los valores de [w, b] por iteración\n",
    "    \"\"\"\n",
    "\n",
    "    historial_costos = []\n",
    "    historial_parametros = []\n",
    "\n",
    "    w_actual = w_inicial\n",
    "    b_actual = b_inicial\n",
    "\n",
    "    for paso in range(iteraciones):\n",
    "        grad_w, grad_b = funcion_gradiente(datos, etiquetas, w_actual, b_actual)\n",
    "\n",
    "        # Actualizamos los parámetros\n",
    "        w_actual = w_actual - tasa_aprendizaje * grad_w\n",
    "        b_actual = b_actual - tasa_aprendizaje * grad_b\n",
    "\n",
    "        # Guardamos historial si no es demasiado grande\n",
    "        if paso < 100000:\n",
    "            costo = funcion_costo(datos, etiquetas, w_actual, b_actual)\n",
    "            historial_costos.append(costo)\n",
    "            historial_parametros.append([w_actual, b_actual])\n",
    "\n",
    "        # Imprimimos el estado cada cierta cantidad de iteraciones\n",
    "        if paso % math.ceil(iteraciones / 10) == 0:\n",
    "            print(f\"Iteración {paso:4}: Costo {historial_costos[-1]:0.2e} \",\n",
    "                  f\"grad_w: {grad_w: 0.3e}, grad_b: {grad_b: 0.3e}  \",\n",
    "                  f\"w: {w_actual: 0.3e}, b: {b_actual: 0.5e}\")\n",
    "\n",
    "    return w_actual, b_actual, historial_costos, historial_parametros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89a22e",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos los parámetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# definimos hyperparámetros\n",
    "iterations = 5000\n",
    "tasa_aprendizaje = 1.0e-2\n",
    "# corremos descenso de gradiente\n",
    "w_final, b_final, J_hist, p_hist = descenso_gradiente(x_train ,y_train, w_init, b_init, tasa_aprendizaje, \n",
    "                                                    iterations, calcular_costo, calcular_gradiente)\n",
    "print(f\"(w,b) encontrados por el descenso de gradiente: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d70750",
   "metadata": {},
   "source": [
    "Vamos a superponer la linea encontrada con los puntos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero calculamos muestras a lo largo de la linea encontrada por el algoritmo\n",
    "muestras_x = np.linspace(x_train.min(), x_train.max()) \n",
    "muestras_y = w_final * muestras_x + b_final\n",
    "plt.scatter(muestras_x, muestras_y, label='predicciones')\n",
    "plt.scatter(x_train, y_train, label='datos entrenamiento')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8ac5f",
   "metadata": {},
   "source": [
    "Como podemos observar en el grafico de arriba. El algoritmo de descenso de gradiente nos permitió encontrar los parametros w y b correctos para ajustar los dos ejemplos de entrenamiento. \n",
    "\n",
    "Ahora vamos a crear un gráfico del costo frente a las iteraciones para observar el progreso en el descenso del gradiente. El costo debe disminuir siempre en ejecuciones exitosas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8136887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar costo. Cambia los valores de inicio y final de iteración para ver distintas partes del proceso. \n",
    "iteracion_inicio = 0   \n",
    "iteracion_final = 500 \n",
    "plt.plot(J_hist[iteracion_inicio:iteracion_final])\n",
    "\n",
    "plt.title(\"Costo vs. iteración\")\n",
    "plt.ylabel('Costo')\n",
    "plt.xlabel('Paso de iteración')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc78886",
   "metadata": {},
   "source": [
    "## Predicciones \n",
    "\n",
    "Ahora que has encontrado los valores óptimos para los parámetros $w$ y $b$, puedes utilizar el modelo para predecir los valores de las viviendas basándote en los parámetros aprendidos. Como era de esperarse, los valores predichos son casi iguales a los valores de entrenamiento para esas mismas viviendas. Además, el valor que no se incluyó en el entrenamiento está en línea con el valor esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20072781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicción para un valor de x de 1.0  {w_final*1.0 + b_final:0.1f}\")\n",
    "print(f\"Predicción para un valor de x de 1.5  {w_final*1.2 + b_final:0.1f}\")\n",
    "print(f\"Predicción para un valor de x de 2.0  {w_final*2.0 + b_final:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e49c7",
   "metadata": {},
   "source": [
    "## Visualización descenso del gradiente\n",
    "\n",
    "La siguiente celda crea una visualización que muestra el progreso del descenso del gradiente durante su ejecución graficando el costo a lo largo de las iteraciones sobre un gráfico de contorno de la función de costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1332ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "graficar_contorno_con_descenso(x_train, y_train, p_hist, ax, calcular_costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831eb957",
   "metadata": {},
   "source": [
    "Arriba, las gráficas de contorno muestran el costo $J(w,b)$ para un rango de $w$ y $b$. Sobre ellas, usando flechas rojas se muestra el camino del descenso del gradiente. Por favor notar lo siguiente:\n",
    "- La trayectoria avanza de forma constante (monótona) hacia su objetivo.\n",
    "- Los pasos iniciales son mucho más grandes que los pasos cerca del objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9343a2",
   "metadata": {},
   "source": [
    "Actividad: Cambia el valor de la taza de aprendizaje y observa el impacto en el proceso de entrenamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
